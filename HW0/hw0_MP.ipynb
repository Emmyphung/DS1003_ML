{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "intro",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Homework 0\n",
    "\n",
    "\n",
    "### Gradescope\n",
    "\n",
    "Name: My (Emmy) Phung <br>\n",
    "\n",
    "NetID: mtp363"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1: Data Science in Python\n",
    "\n",
    "Rate your comfort level with Python:\n",
    "\n",
    "- Expert - I could (or do) get paid for it.\n",
    "- **Good enough to get the job done.**\n",
    "- Mmmm... Haven't used it much, but you know one language, you know them all, right?\n",
    "- Weird - why are you asking about snakes?\n",
    "\n",
    "Rate your comfort level with numpy (http://www.numpy.org/):\n",
    "\n",
    "- I'm pretty proficient in numpy. \n",
    "- **Not so much, but I'm good at matrix/vector stuff in matlab (or\n",
    "something else), and I'm very comfortable with vectorizing mathematical calculations.**\n",
    "- Can't wait to learn!\n",
    "- Not super-excited about programming learning algorithms from scratch -- hasn't somebody else already solved that problem for us?\n",
    "\n",
    "\n",
    "Rate your fluency in data visualization in Python (e.g.\n",
    "matplotlib, bqplot, etc.)\n",
    "\n",
    "- I make great plots.\n",
    "- **With enough googling, I can get the job done.**\n",
    "- I prefer to look at the data numerically, preferably in hex.  ASCII art now and then, but strictly ironically. \n",
    "\n",
    "Which of the following topics are you already familiar with from\n",
    "  machine learning (they are important machine learning topics that you are\n",
    "  assumed to know already for this course):\n",
    "\n",
    "- **Supervised learning framework**\n",
    "- **Cross-validation**\n",
    "- **Overfitting**\n",
    "- **Sample bias**\n",
    "- **Precision/recall, AUC, ROC curves, confusion matrices**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2: Math Experience\n",
    "\n",
    "Which of the following math courses have you taken (i.e. Things that you presumably knew at one point, and could potentially remember with some review):\n",
    "\n",
    "- **Linear algebra (matrix algebra, vector spaces, orthogonal matrices, eigenvalues, projections, span)**\n",
    "- **Linear algebra with proofs**\n",
    "- Real analysis \n",
    "- **Probability theory (e.g. conditional expectations, law of large numbers, central limit theorem)**\n",
    "- **Statistics (bias, variance, confidence intervals, basic parametric probability distributions)**\n",
    "- **Multivariate [differential] calculus (gradients, Jacobians, chain rule)**\n",
    "\n",
    "\n",
    "\n",
    "When you hear or see the following, what do you think?  (Not whether\n",
    "  you already know what's written, but whether you're comfortable with the\n",
    "  notation and/or language.)\n",
    " \n",
    " Let $S$ be the subspace\n",
    "    spanned by the orthonormal vectors $a$ and $b$. Let $p$ be the\n",
    "    projection of the vector $v$ into $S$. Let $r=v-p$ be the residual\n",
    "    vector. Then $r\\perp S$ and $\\left\\{ r,a,b\\right\\} $ form an orthonormal\n",
    "    set.\n",
    "\n",
    "- **You're speaking my language - totally comfortable.**\n",
    "- Familiar, but rusty.  I'll be ready to go by the start of class.\n",
    "- Never properly learned this.  I need to get up to speed.\n",
    "- Wait, this is what I signed up for? \n",
    "\n",
    "When you hear or see the following, what do you think?  (Not whether\n",
    "  you already know what's written, but whether you're comfortable with the\n",
    "  notation and/or language.)\n",
    "\n",
    "Given some data $\\left(x_{1},y_{1}\\right),\\ldots,\\left(x_{n},y_{n}\\right)\\in\\mathbb{R}^{d}\\times\\mathbb{R}$,\n",
    "    the ridge regression solution for regularization parameter $\\lambda>0$\n",
    "    is given by\n",
    "    $$\n",
    "      \\hat{w}=\\operatorname{argmin}_{w\\in\\mathbb{R}^{d}}\\frac{1}{n}\\sum_{i=1}^{n}\\left\\{ w^{T}x_{i}-y_{i}\\right\\} ^{2}+\\lambda\\|w\\|_{2}^{2},\n",
    "    $$\n",
    "    where $\\|w\\|_{2}^{2}=w_{1}^{2}+\\cdots+w_{d}^{2}$ is the square of\n",
    "    the $\\ell_{2}$-norm of $w$.\n",
    "  \n",
    "- **You're speaking my language - totally comfortable.**\n",
    "- Familiar, but rusty.  I'll be ready to go by the start of class.\n",
    "- Never properly learned this.  I need to get up to speed.\n",
    "- Wait, this is what I signed up for? \n",
    "\n",
    "When you hear or see the following, what do you think?  (Not whether\n",
    "  you already know what's written, but whether you're comfortable with the\n",
    "  notation and/or language.):\n",
    "\n",
    "For loss function $\\ell:\\mathcal{Y}\\times\\mathcal{Y}\\to\\mathbb{R}$,\n",
    "    define the risk of a function $f:\\mathcal{X}\\to\\mathcal{X}$ by \n",
    "    $$\n",
    "      R(f)=\\mathbb{E}\\ell\\left(f(x),y\\right),\n",
    "    $$\n",
    "    where the expectation is over $(x,y)\\sim P_{\\mathcal{X}\\times\\mathcal{Y}}$, a distribution\n",
    "    over $\\mathcal{X}\\times\\mathcal{Y}$.\n",
    "    \n",
    "- You're speaking my language - totally comfortable.\n",
    "- **Familiar, but rusty.  I'll be ready to go by the start of class.**\n",
    "- Never properly learned this.  I need to get up to speed.\n",
    "- Wait, this is what I signed up for? \n",
    "\n",
    "When you hear or see the following, what do you think?  (Not whether\n",
    "  you already know what's written, but whether you're comfortable with the\n",
    "  notation and/or language.):\n",
    "\n",
    "If we fix a direction $u\\in\\mathbb{R}^{d}$,\n",
    "    we can compute the directional derivative $f'(x;u)$ as\n",
    "    $$\n",
    "      f'(x;u)=\\lim_{h\\to0}\\frac{f(x+hu)-f(x)}{h}.\n",
    "    $$\n",
    " \n",
    "- **You're speaking my language - totally comfortable.**\n",
    "- Familiar, but rusty.  I'll be ready to go by the start of class.\n",
    "- Never properly learned this.  I need to get up to speed.\n",
    "- Wait, this is what I signed up for? \n",
    "\n",
    "How comfortable are you answering the following question:\n",
    "\n",
    "Verify, just by multiplying out the expressions on the RHS, that\n",
    "    the following completing the square identity is true: For any vectors\n",
    "    $x,b\\in\\mathbb{R}^{d}$ and symmetric invertible matrix $M\\in\\mathbb{R}^{d\\times d}$,\n",
    "    we have\n",
    "    $$\n",
    "      x^{T}Mx-2b^{T}x  =  \\left(x-M^{-1}b\\right)^{T}M(x-M^{-1}b)-b^{T}M^{-1}b\n",
    "    $$\n",
    "    \n",
    "\n",
    "- **So easy. If I had a whiteboard here, I'd do it for you right now.**\n",
    "- Yeah - easy.  I'll have the answer to you in 5 minutes -- I just have to check something on Google first.  \n",
    "- Hmmmm. This will be easy by the first day of class.\n",
    "- :( \n",
    " \n",
    "\n",
    "How comfortable are you answering the following question: \n",
    "\n",
    "Take the gradient of the following w.r.t. $w$:\n",
    "$$\n",
    "      L(w,b,\\xi,\\alpha,\\lambda)=\\frac{1}{2}||w||^{2}+\\frac{c}{n}\\sum_{i=1}^{n}\\xi_{i}+\\sum_{i=1}^{n}\\alpha_{i}\\left(1-y_{i}\\left[w^{T}x_{i}+b\\right]-\\xi_{i}\\right)-\\sum_{i=1}^{n}\\lambda_{i}\\xi_{i}\n",
    "  $$\n",
    "  \n",
    "\n",
    "- **So easy. If I had a whiteboard here, I'd do it for you right now.**\n",
    "- Yeah - easy.  I'll have the answer to you in 5 minutes -- I just have to check something on Google first.  \n",
    "- Hmmmm. This will be easy by the first day of class.\n",
    "- :( \n",
    "\n",
    "\n",
    " How comfortable are you answering the following question: \n",
    "\n",
    "Consider $x_{1},\\ldots,x_{n}$ sampled i.i.d.\n",
    "    from a distribution $P$ on $\\mathbb{R}$. Write $\\mu=\\mathbb{E} x$, for $x\\sim P$.\n",
    "    Show that the mean $\\bar{x}=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}$ is an unbiased\n",
    "    estimate of $\\mu$ (i.e. show that $\\mathbb{E}\\bar{x}=x$). Similarly, show that the sample variance\n",
    "    $\\frac{1}{n-1}\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}$ is an\n",
    "    unbiased estimate for $\\mathbb{Var}\\left(x\\right)$. \n",
    "\n",
    "\n",
    "- So easy. If I had a whiteboard here, I'd do it for you right now.\n",
    "- **Yeah - easy.  I'll have the answer to you in 5 minutes -- I just have to check something on Google first.** \n",
    "- Hmmmm. This will be easy by the first day of class.\n",
    "- :( \n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
